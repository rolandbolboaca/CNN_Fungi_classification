{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7af451",
   "metadata": {},
   "source": [
    "# Fungi Classification Using Macro and Micro Images\n",
    "\n",
    "This notebook implements a CNN-based classification system for fungal species using macro and micro images. The project includes tools for data visualization, model training and evaluation, hyperparameter tuning, and performance benchmarking through confidence intervals and varying train/test splits.\n",
    "\n",
    "! Depnding on the number of images and your PC configuration, the experiments can take longer to complete. Be patient little one! The training process takes longer only!\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "- **Module Imports**  \n",
    "  Loads all necessary Python libraries.\n",
    "\n",
    "- **Hyperparameter and Parameter Configuration**  \n",
    "  Sets key model parameters (e.g., learning rate, batch size, image size, data path).\n",
    "\n",
    "- **CNN Model Construction**  \n",
    "  Builds the convolutional neural network.\n",
    "\n",
    "- **Model Training**  \n",
    "  Trains the CNN on your dataset.\n",
    "\n",
    "- **Run Model (No Hyperparameter Tuning)**  \n",
    "  Executes training and evaluation using predefined parameters.\n",
    "\n",
    "- **Run Model (With Hyperparameter Tuning)**  \n",
    "  Applies automatic tuning to optimize model performance.\n",
    "\n",
    "- **Additional Plots**  \n",
    "  Generates charts for data distributions and model metrics.\n",
    "\n",
    "- **Train/Test Split Experiment**  \n",
    "  Tests model performance across different train/test ratios.\n",
    "\n",
    "- **Confidence Interval Experiment**  \n",
    "  Trains and tests the model 100 times to compute accuracy mean and standard deviation.\n",
    "## Utilized Performance Metrics:\n",
    "\n",
    "## Usage Guide\n",
    "\n",
    "1. **Prepare Your Dataset**  \n",
    "   Place your dataset in the folder: `data/yourfoldername`.\n",
    "\n",
    "2. **Update Configuration**  \n",
    "   In the configuration block, change `data_path` to your dataset folder. Modify other parameters as needed.\n",
    "\n",
    "3. **Optional: Customize the CNN**  \n",
    "   Adjust the architecture in the **CNN Model Construction** block if desired.\n",
    "\n",
    "4. **Initialize the Environment**  \n",
    "   Run all blocks up to and including **Run Model** to load functions and settings.\n",
    "\n",
    "5. **Run Experiments**  \n",
    "   Choose one or more of the following blocks to execute:\n",
    "   - Run Model without Hyperparameter Tuning\n",
    "   - Run Model with Hyperparameter Tuning\n",
    "   - Additional Plots\n",
    "   - Train/Test Split Experiment\n",
    "   - Confidence Interval Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842d413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 06:57:05.841197: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-08 06:57:05.854025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749355025.869160 1813090 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749355025.873222 1813090 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749355025.893286 1813090 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749355025.893314 1813090 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749355025.893315 1813090 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749355025.893317 1813090 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-08 06:57:05.897260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Module Imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "import keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, f1_score, accuracy_score,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425d27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and Parameters\n",
    "\n",
    "# Change paramers as needed. \n",
    "# - To run on other datasets, change 'data_path'\n",
    "# - To use early stopping, set 'EarlyStopping' to True\n",
    "# - To use hyperparameter tuning, set 'tuner' to True\n",
    "\n",
    "params = {\n",
    "    'data_path': 'data/micro',\n",
    "    'image_size': (128, 128),\n",
    "    'batch_size': 32,\n",
    "    'epochs': 30,\n",
    "    'test_split': 0.2,\n",
    "    'validation_split': 0.1,\n",
    "    'random_seed': 42,\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizer': 'adam',\n",
    "    'shuffle': True,\n",
    "    'EarlyStopping': False,\n",
    "    'tuner': False,\n",
    "    'verbose': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2baa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Split Data\n",
    "\n",
    "def get_dataset():\n",
    "    # Load dataset\n",
    "    dataset = image_dataset_from_directory(\n",
    "        params['data_path'],\n",
    "        image_size=params['image_size'],\n",
    "        batch_size=params['batch_size'],\n",
    "        seed=params['random_seed']\n",
    "    )\n",
    "\n",
    "    # Shuffle it before splits\n",
    "    dataset.shuffle(params['shuffle'])\n",
    "\n",
    "    # Split into training, testing and validation\n",
    "    train_ds = dataset.take(int((1 - params['test_split']) * len(dataset)))\n",
    "    test_ds = dataset.skip(int((1 - params['test_split']) * len(dataset)))\n",
    "    test_ds = test_ds.take(int((1 - params['validation_split']) * len(test_ds)))\n",
    "    validation_ds = test_ds.skip(int((1 - params['validation_split']) * len(test_ds)))\n",
    "\n",
    "\n",
    "    class_names = dataset.class_names\n",
    "    class_counts = {cls: len(os.listdir(os.path.join(params['data_path'], cls))) for cls in class_names}\n",
    "\n",
    "    return dataset,train_ds, test_ds, validation_ds, class_names, class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106dd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN Model\n",
    "\n",
    "def build_CNN_model(train_ds, validation_ds, class_names):\n",
    "\n",
    "    def build_model_hp(hp):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Input(shape=(*params['image_size'], 3)))\n",
    "\n",
    "        # First conv layer\n",
    "        model.add(layers.Conv2D(\n",
    "            hp.Int('conv_1', 8, 128, step=16),\n",
    "            (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        # Optional second conv layer\n",
    "        if hp.Boolean('use_conv_2'):\n",
    "            model.add(layers.Conv2D(\n",
    "                hp.Int('conv_2', 8, 128, step=16),\n",
    "                (3, 3), activation='relu'))\n",
    "            model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        # Optional third conv layer\n",
    "        if hp.Boolean('use_conv_3'):\n",
    "            model.add(layers.Conv2D(\n",
    "                hp.Int('conv_3', 8, 128, step=16),\n",
    "                (3, 3), activation='relu'))\n",
    "            model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        # First dense layer\n",
    "        model.add(layers.Dense(\n",
    "            hp.Int('dense_1', 8, 128, step=16),\n",
    "            activation='relu'))\n",
    "\n",
    "        # Optional second dense layer\n",
    "        if hp.Boolean('use_dense_2'):\n",
    "            model.add(layers.Dense(\n",
    "                hp.Int('dense_2', 8, 128, step=16),\n",
    "                activation='relu'))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(layers.Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                hp.Choice('lr', [1e-1, 1e-2, 1e-3])\n",
    "            ),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_model_standard(num_classes):\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(*params['image_size'], 3)),\n",
    "            # layers.Rescaling(1./255, input_shape=input_shape),\n",
    "            layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            #layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            #layers.MaxPooling2D(),\n",
    "            layers.Flatten(),\n",
    "            #layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    if params['tuner']:\n",
    "        # tuner = kt.Hyperband(build_model_hp, objective='val_accuracy', max_epochs=params['epochs'], directory='kt_dir', project_name='cnn_tune')\n",
    "        # tuner.search(train_ds, validation_data=validation_ds, epochs=params['epochs'])\n",
    "        # model = tuner.get_best_models(1)[0]\n",
    "\n",
    "        # model.summary()\n",
    "\n",
    "        tuner = kt.Hyperband(\n",
    "            build_model_hp,\n",
    "            objective='val_accuracy',\n",
    "            max_epochs=params['epochs'],\n",
    "            directory='kt_dir',\n",
    "            project_name='cnn_tune'\n",
    "        )\n",
    "\n",
    "        # Run the hyperparameter search\n",
    "        tuner.search(\n",
    "            train_ds,\n",
    "            validation_data=validation_ds,\n",
    "            epochs=params['epochs']\n",
    "        )\n",
    "\n",
    "        # Retrieve the best model\n",
    "        model = tuner.get_best_models(1)[0]\n",
    "\n",
    "        # Optionally retrieve best hyperparameters\n",
    "        best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "        print(\"Best hyperparameters:\")\n",
    "        print(best_hps.values)\n",
    "        \n",
    "    else:\n",
    "        num_classes = len(class_names)\n",
    "        model = build_model_standard(num_classes = num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d344b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model Training with params\n",
    "def train_model(model, train_ds, validation_ds):\n",
    "\n",
    "    if params['EarlyStopping']:\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        start_train = time.time()\n",
    "\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=validation_ds,\n",
    "            epochs=params['epochs'],\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        end_train = time.time()\n",
    "    else:\n",
    "        early_stop = None\n",
    "\n",
    "        start_train = time.time()\n",
    "\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=validation_ds,\n",
    "            epochs=params['epochs'],\n",
    "            verbose=0\n",
    "        )\n",
    "        end_train = time.time()\n",
    "\n",
    "    print(f\"Training time: {end_train - start_train} seconds\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d8ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics \n",
    "def compute_diagnostic_metrics(true_labels, pred_labels):\n",
    "\n",
    "    labels = np.unique(true_labels)\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=labels)\n",
    "    \n",
    "    TPR_list, TNR_list, PPV_list, NPV_list, F1_list = [], [], [], [], []\n",
    "    per_class_metrics = {}\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        TP = cm[i, i]\n",
    "        FN = cm[i, :].sum() - TP\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "        TPR = TP / (TP + FN) if (TP + FN) > 0 else 0  # Sensitivity\n",
    "        TNR = TN / (TN + FP) if (TN + FP) > 0 else 0  # Specificity\n",
    "        PPV = TP / (TP + FP) if (TP + FP) > 0 else 0  # Precision\n",
    "        NPV = TN / (TN + FN) if (TN + FN) > 0 else 0  # NPV\n",
    "        F1  = (2 * TP) / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0\n",
    "\n",
    "        TPR_list.append(TPR)\n",
    "        TNR_list.append(TNR)\n",
    "        PPV_list.append(PPV)\n",
    "        NPV_list.append(NPV)\n",
    "        F1_list.append(F1)\n",
    "\n",
    "        per_class_metrics[label] = {\n",
    "            'Sensitivity (TPR)': TPR,\n",
    "            'Specificity (TNR)': TNR,\n",
    "            'Positive Predictive Value (PPV)': PPV,\n",
    "            'Negative Predictive Value (NPV)': NPV,\n",
    "            'F1 Score': F1\n",
    "        }\n",
    "\n",
    "    overall = {\n",
    "        'True_Labels': true_labels,\n",
    "        'Predicted_Labels': pred_labels,\n",
    "        'Accuracy': accuracy_score(true_labels, pred_labels),\n",
    "        'Sensitivity (TPR)': np.mean(TPR_list),\n",
    "        'Specificity (TNR)': np.mean(TNR_list),\n",
    "        'Positive Predictive Value (PPV)': np.mean(PPV_list),\n",
    "        'Negative Predictive Value (NPV)': np.mean(NPV_list),\n",
    "        'F1 Score': np.mean(F1_list)\n",
    "    }\n",
    "\n",
    "    return overall, per_class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2225fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate & Metrics \n",
    "\n",
    "def evaluate_model(model, test_ds, verbose=True):\n",
    "\n",
    "    true_labels, pred_labels = [], []\n",
    "    start_test = time.time()\n",
    "    for images, labels in test_ds.unbatch():\n",
    "        preds = model.predict(tf.expand_dims(images, axis=0), verbose=0)\n",
    "        true_labels.append(labels.numpy())\n",
    "        pred_labels.append(np.argmax(preds))\n",
    "    end_test = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    overall, per_class_metrics = compute_diagnostic_metrics(true_labels, pred_labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Testing time: {end_test - start_test} seconds\")\n",
    "    \n",
    "        print(\"Overall Results:\")\n",
    "        for key, value in overall.items():\n",
    "            if key == 'True_Labels' or key == 'Predicted_Labels':\n",
    "                continue\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        print(\"\\n Per Class Diagnostic Metrics:\")\n",
    "        for label, metrics in per_class_metrics.items():\n",
    "            print(f\"\\nClass: {label}\")\n",
    "            for key, value in metrics.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "    return overall, per_class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3acaec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Plots \n",
    "\n",
    "def additional_plots(model, history, class_names, class_counts, results):\n",
    "    \n",
    "    plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.titlesize': 14\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.title('Micro Dataset Class Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.xticks(rotation=30, ha='right', rotation_mode='anchor')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"figs/class_distribution_micro.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(results['True_Labels'], results['Predicted_Labels'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xticks(rotation=30, ha='right', rotation_mode='anchor')\n",
    "    plt.savefig(\"figs/confiusion_matrix_tuner_micro_\" + str(params[\"tuner\"]) + \".pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 571 files belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749355065.584492 1813090 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8336 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749355068.692340 1817084 service.cc:152] XLA service 0x7f6800009850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1749355068.692367 1817084 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 SUPER, Compute Capability 8.9\n",
      "2025-06-08 06:57:48.763198: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1749355069.228664 1817084 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1749355071.490774 1817084 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# Main Function without Hyperparameter Tuning\n",
    "\n",
    "params['tuner'] = False\n",
    "params['test_split'] = 0.2\n",
    "\n",
    "dataset, train_ds, test_ds, validation_ds, class_names, class_counts = get_dataset()\n",
    "\n",
    "model = build_CNN_model(train_ds, validation_ds, class_names)\n",
    "\n",
    "model, history = train_model(model, train_ds, validation_ds)\n",
    "\n",
    "results, per_class_results = evaluate_model(model, test_ds, verbose=params['verbose'])\n",
    "\n",
    "additional_plots(model, history, class_names, class_counts, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model with Hyperparameter Tuning (Run once if you really want optimization)\n",
    "!rm -rf ./kt_dir\n",
    "\n",
    "params['tuner'] = True\n",
    "params['test_split'] = 0.2\n",
    "\n",
    "dataset, train_ds, test_ds, validation_ds, class_names, class_counts = get_dataset()\n",
    "\n",
    "model = build_CNN_model(train_ds, validation_ds, class_names)\n",
    "\n",
    "model, history = train_model(model, train_ds, validation_ds)\n",
    "\n",
    "results = evaluate_model(model, test_ds, verbose=params['verbose'])\n",
    "\n",
    "# additional_plots(model, history, class_names, class_counts, results)\n",
    "\n",
    "params['tuner'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b638d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test percentage experiment\n",
    "\n",
    "params['tuner'] = False\n",
    "params['test_split'] = 0.2\n",
    "\n",
    "exp_results = {}\n",
    "\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "    params['test_split'] = i\n",
    "    print(f\"Train_percent: {1-params['test_split']}, Test_percent: {params['test_split']}\")\n",
    "\n",
    "    dataset, train_ds, test_ds, validation_ds, class_names, class_counts = get_dataset()\n",
    "    model = build_CNN_model(train_ds, validation_ds, class_names)\n",
    "    model, history = train_model(model, train_ds, validation_ds)\n",
    "    results, per_class_results = evaluate_model(model, test_ds, verbose=False)\n",
    "    exp_results[f\"Test_percent: {params['test_split']}\"] = results\n",
    "    # additional_plots(model, history, class_names, class_counts, results)\n",
    "\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfd7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test percentage experiment results\n",
    "\n",
    "x_labels = [f\"[{1 - test:.1f} / {test:.1f}]\" for test in np.arange(0.1, 1.0, 0.1)]\n",
    "accuracies = [result['Accuracy'] for result in exp_results.values()]\n",
    "tprs = [result['Sensitivity (TPR)'] for result in exp_results.values()]\n",
    "tnrs = [result['Specificity (TNR)'] for result in exp_results.values()]\n",
    "ppvs = [result['Positive Predictive Value (PPV)'] for result in exp_results.values()]\n",
    "npvs = [result['Negative Predictive Value (NPV)'] for result in exp_results.values()]\n",
    "f1s = [result['F1 Score'] for result in exp_results.values()]\n",
    "\n",
    "# set all parameters for plot, including text sizes for everything\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.titlesize': 14\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "#plt.bar(x_labels, accuracies)\n",
    "plt.plot(x_labels, accuracies, marker='o', linewidth=2)\n",
    "plt.plot(x_labels, tprs, marker='o', linewidth=2)\n",
    "plt.plot(x_labels, tnrs, marker='o', linewidth=2)\n",
    "plt.plot(x_labels, ppvs, marker='o', linewidth=2)\n",
    "plt.plot(x_labels, npvs, marker='o', linewidth=2)\n",
    "plt.plot(x_labels, f1s, marker='o', linewidth=2)\n",
    "\n",
    "plt.xlabel('Train [%] / Test [%]')\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Clinical Performance vs. Train / Test Percentages')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Accuracy', 'Sensitivity', 'Specificity', 'Positive Predictive Value', 'Negative Predictive Value', 'F1 Score'])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figs/accuracy_percent_plot_micro.pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d512826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance over 100 repetitions of the trials\n",
    "\n",
    "params['tuner'] = False\n",
    "params['test_split'] = 0.2\n",
    "\n",
    "exp_results_trials = {}\n",
    "exp_class_results_trials = {}\n",
    "\n",
    "for i in range(100):\n",
    "    dataset, train_ds, test_ds, validation_ds, class_names, class_counts = get_dataset()\n",
    "    model = build_CNN_model(train_ds, validation_ds, class_names)\n",
    "    model, history = train_model(model, train_ds, validation_ds)\n",
    "    results, per_class_results = evaluate_model(model, test_ds, verbose=False)\n",
    "    exp_results_trials[i] = results\n",
    "    exp_class_results_trials[i] = per_class_results\n",
    "\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance over 100 repetitions of the trials results\n",
    "\n",
    "def get_trials_results(metric, name):\n",
    "    mean_ = np.mean(metric)\n",
    "    std_ = np.std(metric)\n",
    "    \n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Mean name: {mean_}\")\n",
    "    print(f\"Standard Deviation name: {std_}\")\n",
    "\n",
    "print(\"Overall results (Macro averaging)\")\n",
    "\n",
    "accuracies = [result['Accuracy'] for result in exp_results_trials.values()]\n",
    "get_trials_results(accuracies, 'Accuracy')\n",
    "\n",
    "tprs = [result['Sensitivity (TPR)'] for result in exp_results_trials.values()]\n",
    "get_trials_results(tprs, 'Sensitivity (TPR)')\n",
    "\n",
    "tnrs = [result['Specificity (TNR)'] for result in exp_results_trials.values()]\n",
    "get_trials_results(tnrs, 'Specificity (TNR)')\n",
    "\n",
    "ppvs = [result['Positive Predictive Value (PPV)'] for result in exp_results_trials.values()]\n",
    "get_trials_results(ppvs, 'Positive Predictive Value (PPV)')\n",
    "\n",
    "npvs = [result['Negative Predictive Value (NPV)'] for result in exp_results_trials.values()]\n",
    "get_trials_results(npvs, 'Negative Predictive Value (NPV)')\n",
    "\n",
    "f1s = [result['F1 Score'] for result in exp_results_trials.values()]\n",
    "get_trials_results(f1s, 'F1 Score')\n",
    "\n",
    "print(\"Overall results (Micro averaging)\")\n",
    "\n",
    "\n",
    "# Collect all metric values per class\n",
    "all_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for trial_idx, trial in exp_class_results_trials.items():\n",
    "    for class_id, metrics in trial.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            all_metrics[class_id][metric_name].append(value)  \n",
    "\n",
    "# Compute and print mean and std for each class and metric\n",
    "for class_id in all_metrics.keys():\n",
    "    print(f\"Class: {class_id}\")\n",
    "    for metric_name, values in all_metrics[class_id].items():\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        print(f\"  {metric_name}: {mean} ± {std}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
